{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Definitions"],"metadata":{"id":"bEKNc5s6HUor"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"iYIsnenE7Rn_"},"outputs":[],"source":["# !git clone https://github.com/Text-Mining/Useful-Corpora-for-Text-Mining-in-Persian-Language.git\n","# !unrar x '/content/Useful-Corpora-for-Text-Mining-in-Persian-Language/News/FarsNews 97/farsnews.part01.rar'\n","!pip install hazm"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import json\n","import ast\n","import math\n","from scipy import spatial\n","from threading import Thread\n","import hazm as hzm\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"kZSvaIj8BSGG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def normalizer(data):\n","    \"\"\"\n","    data: a row of the dataframe\n","    \"\"\"\n","    normalizer = hzm.Normalizer()\n","    return normalizer.normalize(data)\n","\n","\n","def stemmer(data):\n","    stemmer = hzm.Stemmer()\n","    stem_list = []\n","    for i in data:\n","        stem_list.append(stemmer.stem(i))\n","    return stem_list\n","\n","\n","def lemma(text_tokens):\n","    lemmatizer = hzm.Lemmatizer()\n","    temp = []\n","    for word in text_tokens:\n","        temp.append(lemmatizer.lemmatize(word))\n","\n","    return temp\n","\n","\n","def removeStopWords(text_tokens):\n","    tokens_without_sw = [\n","        word for word in text_tokens if not word in hzm.stopwords_list()\n","    ]\n","    return tokens_without_sw\n","\n","\n","def remove_punctuations(text_tokens):\n","    punctuations_list = [\n","        \"،\",\n","        \".\",\n","        \":\",\n","        \"؛\",\n","        \"؟\",\n","        \"!\",\n","        \"'\",\n","        \"\\\\\",\n","        \"/\",\n","        \"-\",\n","        \"ـ\",\n","        \"+\",\n","        \"=\",\n","        \"*\",\n","        \",\",\n","        \"٪\",\n","        \"$\",\n","        \"#\",\n","        \"@\",\n","        \"÷\",\n","        \"<\",\n","        \">\",\n","        \"|\",\n","        \"}\",\n","        \"{\",\n","        \"[\",\n","        \"]\",\n","        \")\",\n","        \"(\",\n","        \"…\",\n","    ]\n","    delimiters_list = [\n","        \"،\",\n","        \".\",\n","        \":\",\n","        \"؛\",\n","        \"؟\",\n","        \"!\",\n","        \"'\",\n","        \"\\\\\",\n","        \"/\",\n","        \"-\",\n","        \"ـ\",\n","        \",\",\n","        \"|\",\n","        \"}\",\n","        \"{\",\n","        \"[\",\n","        \"]\",\n","        \")\",\n","        \"(\",\n","        \"…\",\n","    ]\n","\n","    tokens_without_punc = []\n","    for token in text_tokens:\n","        if token not in punctuations_list:\n","            \"\"\"\n","            the following for-loop is to replace \n","            the punctuations appearing in the middle\n","            of tokens with a space so we can later\n","            split the tokens by space and separately\n","            extract the words\n","            \"\"\"\n","            for delimiter in delimiters_list:\n","                token = token.replace(delimiter, \" \")\n","\n","            for word in token.split():\n","                tokens_without_punc.append(word.strip())\n","\n","    return tokens_without_punc\n","\n","\n","def preprocess_pipeline(\n","    df,\n","    normalize_flag=True,\n","    remove_stop_words_flag=False,\n","    remove_punctuations_flag=False,\n","    lemmatize_flag=False,\n","    stemmer_flag=False,\n","    show_logs=False,\n","):\n","    \"\"\"\n","    input text \n","        ↳ [normalize]\n","            ↳ tokenize\n","                ↳ [remove punctuations] \n","                    ↳ [remove stop words]\n","                        ↳ [lemmatize]\n","                            ↳ [stemmer]\n","                                ↳ output text\n","    \"\"\"\n","    df[\"preprocessed\"] = None\n","    for index in df.index:\n","        text = df.loc[index, \"NewsBody\"]\n","        if normalize_flag:\n","            text = normalizer(df[\"NewsBody\"][index])\n","\n","        text_tokens = hzm.word_tokenize(text)\n","\n","        if remove_punctuations_flag:\n","            text_tokens = remove_punctuations(text_tokens)\n","\n","        if remove_stop_words_flag:\n","            text_tokens = removeStopWords(text_tokens)\n","\n","        if lemmatize_flag:\n","            text_tokens = lemma(text_tokens)\n","\n","        if stemmer_flag:\n","            text_tokens = stemmer(text_tokens)\n","        \n","        df[\"preprocessed\"][index] = \"/\".join(text_tokens)\n","\n","        if show_logs:\n","            print(f\"Preprocessed {index}\")\n","\n","    return df\n","\n","\n","def invert_indexing(df):\n","    terms = []\n","    inverted_index = {\n","        \"Term\": [],\n","        \"DocID_Ferquency\": []\n","    }\n","\n","    for index in df.index:\n","        text_tokens = df.loc[index, \"preprocessed\"]\n","        terms.extend(list(set(text_tokens.split(\"/\"))))\n","    \n","    terms = set(terms)\n","    \n","    print(len(terms))\n","\n","    kl = 0\n","    for token in terms:\n","        each_term_per_document_frequency = {}\n","        for index in df.index:\n","            text_tokens = df.loc[index, \"preprocessed\"]\n","            news_body_array = text_tokens.split(\"/\")\n","            if(token in set(news_body_array)):\n","                count = news_body_array.count(token)\n","                # each_term_per_document_frequency.setdefault(index, 0)\n","                each_term_per_document_frequency[index] = count\n","        inverted_index[\"Term\"].append(token)\n","        inverted_index[\"DocID_Ferquency\"].append(each_term_per_document_frequency)                \n","        # if show_logs:\n","        kl += 1\n","        print(f\"Inverted indexing {(kl/len(terms)*100)} %\")\n","\n","    return inverted_index\n","\n","\n","\n","def retrieve_documents(preprocessed_df, inverted_index, query):\n","    docs_titles = []\n","    docs_index = inverted_index.get(query, [])\n","    for doc_index in docs_index:\n","        docs_titles.append(preprocessed_df.loc[doc_index, \"title\"])\n","\n","    return docs_titles\n","\n","\n","def get_query(preprocessed_df, inverted_index, args):\n","    query = input(\"Enter your query: \").strip()\n","    while query != \"\":\n","        # the end condition is when the\n","        # user enters an empty string\n","        output = []\n","        query_df = pd.DataFrame({\"content\": [query], \"preprocessed\": [\"\"],})\n","        preprocessed_query = preprocess_pipeline(query_df, True, True, True, True, True, True)\n","        processed_query = preprocessed_query[0][\"preprocessed\"][0]\n","        print(f\"Processed query: {processed_query}\")\n","\n","        docs_titles = retrieve_documents(preprocessed_df, inverted_index, query)\n","        print(docs_titles)\n","        print(f\"Retrieved {len(docs_titles)} documents\")\n","\n","        query = input(\"\\nEnter your query: \").strip()\n","\n","\n","def term_frequency(method, word_dictionary):\n","    if(method == 'n'):\n","        return word_dictionary['term']['doc_Id']\n","    if(method == 'l'):\n","        return (1 + math.log(word_dictionary['term']['doc_Id'], 10))\n","    if(method == 'a'):\n","        return (0.5 + ((0.5 * word_dictionary['term']['doc_Id'])/(word_dictionary['term']['*']).max))\n","    # if(method == ''):\n","    #     return ()\n","\n","\n","\n","def tf_idf (word_dictionary, term, doc_Id, number_of_documents):\n","    tf = term_frequency(word_dictionary)\n","    idf = math.log(number_of_documents/len(word_dictionary['term']), 10)\n","    return tf * idf \n","\n","def boolean_model (word_dictionary, number_of_documents):\n","    existance_matrix = np.zero(np.zeros((len(word_dictionary['term']), number_of_documents)))\n","    for i, token in enumerate(word_dictionary['term']):\n","        for j, doc_Id in enumerate(word_dictionary['doc_Id'] in keys):\n","            existance_matrix[i, j] = 1\n","    \n","    return existance_matrix\n","\n","\n","def ranking(vector_query, vector_doc_tf_idf, k):\n","    cosine_distances = []\n","\n","    for i in range(vector_doc_tf_idf.shape[0]):\n","        try:\n","            z = spatial.distance.cosine(vector_doc_tf_idf[i], vector_query)\n","            cosine_distances.append(z.item())\n","        except Exception as e:\n","            print(e)\n","\n","    cosine_distances = np.array(cosine_distances)\n","\n","    top_matches_indices = np.argsort(cosine_distances, axis=0)[:k]\n","    return top_matches_indices, cosine_distances[top_matches_indices]\n","\n","\n"],"metadata":{"id":"uYtIVv1eNdox"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Codes"],"metadata":{"id":"NloMp4raLF5D"}},{"cell_type":"code","source":["data_dic = {\n","    \"Id\": [],\n","    \"CategoryEn1\": [],\n","    \"CategoryFa1\": [],\n","    \"CategoryEn2\": [],\n","    \"CategoryFa2\": [],\n","    \"NewsDate\": [],\n","    \"NewsTitle\": [],\n","    \"NewsSummary\": [],\n","    \"NewsBody\": [],\n","}\n","\n","comment = {\n","    \"newsID\": [],\n","    \"id\": [],\n","    \"parentID\": [],\n","    \"parentName\": [],\n","    \"name\": [],\n","    \"createDate\": [],\n","    \"persianCreateDate\": [],\n","    \"text\": [],\n","}\n","\n","id_news = 0\n","with open('/content/farsnews.json', encoding='utf-8-sig') as document:\n","    for line in document:\n","        # data = json.loads(line)\n","        # print(data[\"NewsTitle\"])\n","        # x = data[\"NewsTitle\"].encode('utf-8-sig')\n","        # z = x.decode('utf-8')\n","        # print(z)\n","        # break\n","        # print(z['CategoryPanel'])\n","        \n","        data = json.loads(line)\n","        # print(data)\n","        # # data = data.decode('utf-8')\n","\n","        # # ID\n","        data_dic[\"Id\"].append(id_news)\n","        \n","\n","        # Category panel\n","        try:\n","            cp = data[\"CategoryPanel\"]\n","\n","            data_dic['CategoryEn1'].append(cp[0][\"CategoryEn\"].encode('utf-8-sig').decode('utf-8'))\n","            data_dic['CategoryEn2'].append(cp[1][\"CategoryEn\"].encode('utf-8-sig').decode('utf-8'))\n","\n","            data_dic['CategoryFa1'].append(cp[0][\"CategoryFa\"].encode('utf-8-sig').decode('utf-8'))\n","            data_dic['CategoryFa2'].append(cp[1][\"CategoryFa\"].encode('utf-8-sig').decode('utf-8'))\n","\n","        except:\n","            try:\n","                ce = data[\"CategoryEn\"].encode('utf-8-sig').decode('utf-8')\n","                cf = data[\"CategoryFa\"].encode('utf-8-sig').decode('utf-8')\n","                data_dic['CategoryEn1'].append(ce)\n","                data_dic['CategoryFa1'].append(cf)\n","\n","                data_dic['CategoryEn2'].append(\"None\")\n","                data_dic['CategoryFa2'].append(\"None\")\n","            except:\n","                cp = \"None\"\n","                data_dic['CategoryEn1'].append(cp)\n","                data_dic['CategoryEn2'].append(cp)\n","                data_dic['CategoryFa1'].append(cp)\n","                data_dic['CategoryFa2'].append(cp)\n","\n","\n","        # News Date\n","        try:\n","            newsdate = str(data[\"NewsDate\"]).encode('utf-8-sig').decode('utf-8').rstrip()\n","            data_dic[\"NewsDate\"].append(newsdate)\n","        except:\n","            data_dic[\"NewsDate\"].append(\"None\")\n","        \n","        # News Title\n","\n","        try:\n","            newstitle = str(data[\"NewsTitle\"].encode('utf-8-sig').decode('utf-8'))\n","            data_dic[\"NewsTitle\"].append(newstitle)\n","        except:\n","            data_dic[\"NewsTitle\"].append(\"None\")\n","        \n","        # News Summary\n","        try:\n","            newssummery = str(data[\"NewsSummary\"].encode('utf-8-sig').decode('utf-8'))\n","            data_dic[\"NewsSummary\"].append(newssummery)\n","        except:\n","            data_dic[\"NewsSummary\"].append(\"None\")\n","        \n","\n","        # News Body\n","        try:\n","            newsbody = str(data[\"NewsBody\"].encode('utf-8-sig').decode('utf-8'))\n","            data_dic[\"NewsBody\"].append(newsbody)\n","        except:\n","            data_dic[\"NewsBody\"].append(\"None\")\n","\n","        # Comment\n","\n","        # try:\n","        #     comments = data[\"GetComments\"][\"CommentsJsonArray\"]\n","        #     for i in range(0, len(comments)):\n","        #         comment[\"newsID\"].append(id_news)\n","        #         comment[\"id\"].append(comment['id'])\n","        #         comment[]\n","                \n","        # except:\n","        #     pass\n","\n","        \n","        \n","        # Update id news for table\n","        id_news = id_news + 1\n","\n","\n","        # Show Status\n","\n","        if id_news % 200 == 0:\n","            # print(data_dic)\n","            print(f\"{id_news} completed!\")\n","\n","\n","        \n","# print(data_dic)\n","# print(\"----\")\n","data_frame = pd.DataFrame(data_dic)\n","data_frame.to_csv(\"News.csv\")"],"metadata":{"id":"q5w4MsUz8y3J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datas = pd.read_csv(\"/content/data.csv\")\n","datas.head(10)"],"metadata":{"id":"POS88STXCtql"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["news = pd.read_csv(\"/content/gdrive/MyDrive/News.csv\")\n","x = preprocess_pipeline(news)\n","# data_frame.to_csv(\"News.csv\")"],"metadata":{"id":"vbhgBQr2Ohva"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_dic = invert_indexing(x, True)\n","data_frame = pd.DataFrame(data_dic)\n","data_frame.to_csv(\"News.csv\")"],"metadata":{"id":"D2if0J95l-kh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["news = pd.read_csv(\"/content/gdrive/MyDrive/PreProcessedNews.csv\")\n","news = news.iloc[:1000,:]\n","data_dic = invert_indexing(news)\n","\n","data_frame = pd.DataFrame().from_dict(data_dic)\n","data_frame.to_csv(\"/content/gdrive/MyDrive/Posting_Lists.csv\")"],"metadata":{"id":"1_PX1okPUYqb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["news = pd.read_csv(\"/content/gdrive/MyDrive/DocIPosting_Lists.csv\")\n","news.head(100)\n"],"metadata":{"id":"Mb1RCOTa3LxS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"9QipmrjeHSuo"}}]}